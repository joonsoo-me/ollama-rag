# Local RAG (App with n8n Orchestration)
An Open AI compatible proxy app that preserves JSONL streaming while delegating Retrieval-Augmented Generation (RAG) preparation and post-upsert to n8n. Drop-in endpoint replacement for existing Ollama clients; switch the base URL to the app and get consistent context-aware answers with minimal client changes.

## ğŸ“‹ Prerequisites / ì‚¬ì „ ìš”êµ¬ì‚¬í•­

### Required Services / í•„ìˆ˜ ì„œë¹„ìŠ¤
- **Ollama Server**: Running and accessible (e.g., `http://localhost:11434`)
  - ì„¤ì¹˜: [ollama.com](https://ollama.com) ë˜ëŠ” `curl -fsSL https://ollama.com/install.sh | sh`
  - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: `ollama pull qwen2:7b` (ë˜ëŠ” ì›í•˜ëŠ” ëª¨ë¸)

### Development Environment / ê°œë°œ í™˜ê²½
- **Node.js 20+**: [nodejs.org](https://nodejs.org) ë˜ëŠ” `winget install OpenJS.NodeJS`
- **PowerShell 5.1+**: Windowsì— ê¸°ë³¸ ì„¤ì¹˜ë¨ / Pre-installed on Windows
- **Git**: [git-scm.com](https://git-scm.com) ë˜ëŠ” `winget install Git.Git`

### Optional for Full RAG / RAG ì „ì²´ ê¸°ëŠ¥ìš© (ì„ íƒì‚¬í•­)
- **n8n Workflow Engine**: RAG orchestrationì„ ìœ„í•´ / For RAG orchestration
  - Docker: `docker run -p 5678:5678 n8nio/n8n`
  - ë˜ëŠ” [n8n.io ì„¤ì¹˜ ê°€ì´ë“œ](https://docs.n8n.io/hosting/installation/)
- **Vector Database (Qdrant)**: ë¬¸ì„œ ì €ì¥ìš© / For document storage
  - Docker: `docker run -p 6333:6333 qdrant/qdrant`
- **Text Embeddings Inference (TEI)**: ì„ë² ë”© ìƒì„±ìš© / For embedding generation
  - ë˜ëŠ” ë‹¤ë¥¸ ì„ë² ë”© ì„œë¹„ìŠ¤ (OpenAI API, ë¡œì»¬ ëª¨ë¸ ë“±)

### Network Requirements / ë„¤íŠ¸ì›Œí¬ ìš”êµ¬ì‚¬í•­
- **ì¸í„°ë„· ì—°ê²°**: npm íŒ¨í‚¤ì§€ ì„¤ì¹˜ìš© / Internet connection for npm packages
- **í¬íŠ¸ ì ‘ê·¼**: 11434 (ì•±), 11434 (Ollama), 5678 (n8n), 6333 (Qdrant)
- **ë°©í™”ë²½ ì„¤ì •**: í•„ìš”í•œ í¬íŠ¸ë“¤ì´ ì—´ë ¤ìˆì–´ì•¼ í•¨ / Required ports should be open

## ğŸš€ Current Implementation Status / í˜„ì¬ êµ¬í˜„ ìƒíƒœ

### âœ… Completed Features / ì™„ë£Œëœ ê¸°ëŠ¥
- **Core Gateway**: Ollama-compatible proxy with full JSONL streaming
- **TypeScript Support**: Complete TypeScript setup with build scripts  
- **RAG Integration**: n8n webhook integration for prepare and upsert
- **Streaming Relay**: Byte-for-byte NDJSON passthrough from Ollama
- **Context Injection**: Automatic system message and context block insertion
- **Error Handling**: Graceful fallback when RAG services are unavailable
- **Environment Configuration**: All major settings via environment variables

### âœ… Recently Completed / ìµœê·¼ ì™„ë£Œ
- **Health Check Endpoints**: `/healthz` and `/readyz` with upstream service monitoring
- **Docker Multi-stage Build**: Optimized containerization with security best practices
- **CI/CD Pipeline**: Complete GitHub Actions workflow with GHCR publishing and security scanning

### ğŸ”„ In Progress / ì§„í–‰ ì¤‘
- Metrics and observability improvements
- Security scanning integration testing

### ğŸ“‹ Planned / ê³„íšë¨
- n8n workflow templates
- Portainer deployment stack refinements
- Advanced security features (auth, rate limiting)

## ğŸ“„ License / ë¼ì´ì„¼ìŠ¤

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

### Open Source Dependencies / ì˜¤í”ˆì†ŒìŠ¤ ì˜ì¡´ì„±
This project uses the following open source packages:
- **Express.js** - MIT License
- **undici** - MIT License  
- **TypeScript** - Apache-2.0 License
- **Node.js** - MIT License

### Contributing / ê¸°ì—¬í•˜ê¸°
Contributions are welcome! Please feel free to submit a Pull Request.

## Quick Test - í˜„ì¬ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ / Currently Testable
âœ… **ì„œë²„ê°€ í¬íŠ¸ 11434ì—ì„œ ì‹¤í–‰ ì¤‘** / **Server running on port 11434**

After deployment, replace your client's Ollama base URL with the app URL (port 11434). Or test with curl from PowerShell:

```powershell
# í˜„ì¬ ì‘ë™í•˜ëŠ” í…ŒìŠ¤íŠ¸ / Currently working test âœ…
curl.exe http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
	"model": "qwen3:0.6b",
	"messages": [ { "role": "user", "content": "Hello, how are you?" } ],
	"stream": true
}'
```

âœ… You should receive streaming NDJSON, the same format as native Ollama.

**í—¬ìŠ¤ ì²´í¬ í…ŒìŠ¤íŠ¸ / Health check test:**
```powershell
# ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬ / Basic health check
curl.exe http://localhost:11434/healthz

# ì¤€ë¹„ ìƒíƒœ ì²´í¬ (ì—…ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ í¬í•¨) / Readiness check (including upstream services)  
curl.exe http://localhost:11434/readyz
```

**ì‹¤ì œ RAG ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ / RAG functionality test:**
- N8N_PREPARE_URLê³¼ N8N_UPSERT_URLì„ ì„¤ì •í•˜ë©´ RAG ê¸°ëŠ¥ì´ í™œì„±í™”ë©ë‹ˆë‹¤ / Set N8N_PREPARE_URL and N8N_UPSERT_URL to enable RAG functionality
- í˜„ì¬ëŠ” n8n ì—†ì´ë„ ê¸°ë³¸ Ollama í”„ë¡ì‹œë¡œ ì‘ë™í•©ë‹ˆë‹¤ / Currently works as basic Ollama proxy without n8nent Implementation Status
âœ… **POST `/api/chat`** - **ì™„ì „ êµ¬í˜„ë¨ / Fully Implemented**
- Request: Ollama-compatible `{ model, messages[], stream=true, options?, session_id?, thread_id? }`
- Behavior: âœ… Streams NDJSON lines identical to Ollama's `/api/chat`. Adds a system message and, if available, a context block from n8n.

âœ… **POST `/api/embeddings`** - **ì™„ì „ êµ¬í˜„ë¨ / Fully Implemented**
- Behavior: âœ… Pass-through proxy to `OLLAMA_URL/api/embeddings`.

âœ… **Health Endpoints** - **ì™„ì „ êµ¬í˜„ë¨ / Fully Implemented**
- `/healthz` â†’ `200` (no deps): Basic health check without dependencies
- `/readyz` â†’ checks upstream services: Validates Ollama and n8n service availabilityshile delegating Retrieval-Augmented Generation (RAG) preparation and post-upsert to n8n. Drop-in endpoint replacement for existing Ollama clients; switch the base URL to the app and get consistent context-aware answers with minimal client changes.

## ğŸš€ Current Implementation Status / í˜„ì¬ êµ¬í˜„ ìƒíƒœ

### âœ… Completed Features / ì™„ë£Œëœ ê¸°ëŠ¥
- **Core Gateway**: Ollama-compatible proxy with full JSONL streaming
- **TypeScript Support**: Complete TypeScript setup with build scripts  
- **RAG Integration**: n8n webhook integration for prepare and upsert
- **Streaming Relay**: Byte-for-byte NDJSON passthrough from Ollama
- **Context Injection**: Automatic system message and context block insertion
- **Error Handling**: Graceful fallback when RAG services are unavailable
- **Environment Configuration**: All major settings via environment variables

### ğŸ”„ In Progress / ì§„í–‰ ì¤‘
- Health check endpoints (`/healthz`, `/readyz`)
- Docker containerization
- Metrics and observability

### ğŸ“‹ Planned / ê³„íšë¨
- n8n workflow templates
- Portainer deployment stack
- CI/CD pipeline for container builds
- Advanced security features (auth, rate limiting)

## Why this design
- Gateway handles real-time JSONL streaming to clients. n8n focuses on RAG prep (embed â†’ search â†’ rerank) and post-processing (re-embedding/upsert), which donâ€™t need streaming.
- Clean separation improves reliability, scalability, and maintainability. Swap RAG components (Qdrant/TEI today; Elastic/ColBERT or GraphRAG tomorrow) without touching clients.
- Easy to ship end-to-end: GitHub â†’ GHCR (or Docker Hub) â†’ Portainer Stack.

## Repository layout
```
repo root
â”œâ”€ app/                       # Node/Express proxy â€“ Ollama-compatible âœ… êµ¬í˜„ë¨ / Implemented
â”‚  â”œâ”€ Dockerfile              # ğŸ”„ êµ¬í˜„ ì˜ˆì • / To be implemented
â”‚  â”œâ”€ package.json            # âœ… TypeScript ì„¤ì • ì™„ë£Œ / TypeScript configured
â”‚  â”œâ”€ tsconfig.json           # âœ… TypeScript ì„¤ì • ì™„ë£Œ / TypeScript configured
â”‚  â””â”€ src/
â”‚     â””â”€ server.ts            # âœ… í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ ì™„ë£Œ / Core functionality implemented
â”œâ”€ workflows/                 # n8n exported workflows (import in n8n)
â”‚  â””â”€ rag-prepare-upsert.json # ğŸ”„ êµ¬í˜„ ì˜ˆì • / To be implemented
â”œâ”€ compose/
â”‚  â”œâ”€ docker-compose.yml      # ğŸ”„ êµ¬í˜„ ì˜ˆì • / To be implemented - Optional: compose up for local/edge
â”‚  â””â”€ portainer-stack.yml     # ğŸ”„ êµ¬í˜„ ì˜ˆì • / To be implemented - Use in Portainer as a Git/Repository stack
â””â”€ README.md                  # âœ… ë¬¸ì„œí™”ë¨ / Documented
```

## How it works
1) Client POSTs to the app at `/api/chat` (same payloads as Ollama).  
2) App extracts the last user message and calls n8n webhook `/webhook/rag/prepare` to get retrieved context and document refs.  
3) App composes messages: system prompt + context block + original conversation.  
4) App calls Ollama `/api/chat?stream=true` and relays JSONL chunks unchanged to the client.  
5) After completion, app calls n8n `/webhook/rag/upsert` with the Q/A and refs to store embeddings and upsert into the vector DB.

Key properties:
- Full JSONL streaming preserved (`Content-Type: application/x-ndjson`).
- Embeddings endpoint `/api/embeddings` is proxied pass-through to Ollama by default.
- Policy knobs (top-k, rerank top-n, thresholds, fallback) are environment-driven.

## Runtime configuration (env) - í˜„ì¬ êµ¬í˜„ ìƒíƒœ / Current Implementation Status
- âœ… **OLLAMA_URL**: Base URL of your Ollama server (e.g., http://ollama:11434) - **êµ¬í˜„ë¨ / Implemented**
- âœ… **N8N_PREPARE_URL**: n8n webhook for RAG prepare (e.g., http://n8n:5678/webhook/rag/prepare) - **êµ¬í˜„ë¨ / Implemented**
- âœ… **N8N_UPSERT_URL**: n8n webhook for RAG upsert (e.g., http://n8n:5678/webhook/rag/upsert) - **êµ¬í˜„ë¨ / Implemented**
- âœ… **GATEWAY_BIND_PORT**: Port to expose (default 11434 for Ollama compatibility) - **êµ¬í˜„ë¨ / Implemented**
- âœ… **RAG_TOPK**: Retrieval top-k (default 12) - **êµ¬í˜„ë¨ / Implemented**
- âœ… **RERANK_TOPN**: Rerank top-n (default 5) - **êµ¬í˜„ë¨ / Implemented**
- ğŸ”„ **SCORE_THRESHOLD**: Optional min score cutoff for context - **êµ¬í˜„ ì˜ˆì • / To be implemented**

Optional ideas (not required to run):
- ğŸ”„ **MODEL_ROUTE_TABLE**: JSON map for intent-based model routing - **êµ¬í˜„ ì˜ˆì • / To be implemented**
- ğŸ”„ Auth headers/tokens for upstreams; rate limits; circuit breaker toggles - **êµ¬í˜„ ì˜ˆì • / To be implemented**

## n8n workflow
Import the single JSON file `workflows/rag-prepare-upsert.json` into n8n and publish it.

This workflow exposes two endpoints:

1) POST `/webhook/rag/prepare`
	 - Input: `{ query, session_id?, thread_id?, topk?, topn? }`
	 - Performs: TEI embed â†’ Qdrant search â†’ TEI rerank â†’ returns minimal context
	 - Output (example):
		 ```json
		 {
			 "context": [
				 { "id": "doc-1", "text": "...chunk text...", "score": 0.81 },
				 { "id": "doc-7", "text": "...", "score": 0.75 }
			 ],
			 "docRefs": [ { "id": "doc-1", "score": 0.81 }, { "id": "doc-7", "score": 0.75 } ]
		 }
		 ```

2) POST `/webhook/rag/upsert`
	 - Input: `{ session_id?, thread_id?, question, answer, refs: [{id,score}] }`
	 - Performs: embed new content â†’ upsert into vector DB â†’ returns summary/status

Tip: Register n8n Credentials for Qdrant and TEI; use them from HTTP Request nodes.

## Deploy
### Option A â€” Portainer Stack (recommended)
- Point Portainer to this repository and select `compose/portainer-stack.yml`.
- Set env vars (OLLAMA_URL, N8N_PREPARE_URL, N8N_UPSERT_URL, etc.) in the stack UI.
- Deploy; the app will expose port 11434 by default.

### Option B â€” Docker Compose
- Use `compose/docker-compose.yml` to run locally or on an edge node.
- Ensure the environment values match your upstream services (Ollama/n8n/Qdrant/TEI).

## Local development (app) - í˜„ì¬ ê°œë°œ í™˜ê²½ / Current Development Environment
- Node.js 20+ âœ… **ì§€ì›ë¨ / Supported**
- TypeScript âœ… **ì™„ì „ ì„¤ì •ë¨ / Fully configured**
- Install and run:
	```powershell
	cd app
	npm ci              # ì˜ì¡´ì„± ì„¤ì¹˜ / Install dependencies
	npm run build       # TypeScript ì»´íŒŒì¼ / Compile TypeScript âœ…
	npm run start       # í”„ë¡œë•ì…˜ ì‹¤í–‰ / Production start âœ…
	npm run start:ts    # ê°œë°œ ì‹¤í–‰ / Development start âœ…
	npm run dev         # Watch ëª¨ë“œ / Watch mode âœ…
	```
- The server listens on `GATEWAY_BIND_PORT` (11434 by default). âœ… **ì‘ë™ í™•ì¸ë¨ / Verified working**

## API (app)
POST `/api/chat`
- Request: Ollama-compatible `{ model, messages[], stream=true, options?, session_id?, thread_id? }`
- Behavior: Streams NDJSON lines identical to Ollamaâ€™s `/api/chat`. Adds a system message and, if available, a context block from n8n.

POST `/api/embeddings`
- Behavior: Pass-through proxy to `OLLAMA_URL/api/embeddings`.

## Quick test
After deployment, replace your clientâ€™s Ollama base URL with the app URL (port 11434). Or test with curl from PowerShell:

```powershell
curl.exe http://localhost:11434/api/chat -H "Content-Type: application/json" -d '{
	"model": "qwen2:7b",
	"messages": [ { "role": "user", "content": "Share the latest install guide for Product A" } ],
	"stream": true
}'
```

You should receive streaming NDJSON, the same format as native Ollama.

## Operations tips
- Streaming always handled by the app; n8n stays synchronous (JSON in/out).
- Fallback fast: if TEI/Qdrant fails, gateway continues without RAG context.
- Trim context blocks (budgeting) and prefer safety filters against prompt injection.
- Log stage timings, doc IDs, scores, and final lengths for observability.
- Keep-Alive and compression for external TEI/Qdrant endpoints to reduce latency.

## CI/CD âœ… êµ¬í˜„ ì™„ë£Œ / Implemented
Complete automated build and publish pipeline using GitHub Actions:
- **Triggers**: Push to `main`/`develop` branches, tags (`v*`), and pull requests
- **Testing**: TypeScript compilation and build verification
- **Multi-platform builds**: Supports `linux/amd64` and `linux/arm64`
- **GHCR Publishing**: Automatic container image publishing to `ghcr.io/<owner>/ollama-rag-app`
- **Security Scanning**: Trivy vulnerability scanning integration
- **Auto-tagging**: Semantic versioning, branch names, commit SHA, and `latest` tags

### Available Images / ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€
```bash
# ìµœì‹  ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸° / Pull latest image
docker pull ghcr.io/bear8203/ollama-rag-app:latest

# íŠ¹ì • ë²„ì „ / Specific version
docker pull ghcr.io/bear8203/ollama-rag-app:v1.0.0
```

## ğŸ› ï¸ Development Scripts / ê°œë°œ ìŠ¤í¬ë¦½íŠ¸

```powershell
# í”„ë¡œì íŠ¸ ì„¤ì • / Project Setup
cd app
npm ci                    # ì˜ì¡´ì„± ì„¤ì¹˜ / Install dependencies

# ê°œë°œ ëª¨ë“œ / Development Mode  
npm run dev              # TypeScript watch ëª¨ë“œ / TypeScript watch mode
npm run start:ts         # TypeScript ì§ì ‘ ì‹¤í–‰ / Direct TypeScript execution

# í”„ë¡œë•ì…˜ ë¹Œë“œ / Production Build
npm run build            # TypeScript â†’ JavaScript ì»´íŒŒì¼ / Compile TypeScript to JavaScript
npm run start            # ì»´íŒŒì¼ëœ JavaScript ì‹¤í–‰ / Run compiled JavaScript

# íƒ€ì… ì²´í¬ / Type Checking
npx tsc --noEmit         # íƒ€ì… ì—ëŸ¬ë§Œ ì²´í¬ / Check types only
```

## Roadmap ideas (short list)
- Queryâ†’context cache (short TTL) + embedding LRU cache to reduce latency
- Dynamic top-k/top-n based on query features and rerank score distribution
- Conversation windows + vector recall of relevant past turns
- Trust/freshness-weighted rerank and multilingual RAG (translate query-only)
- Simple model routing via `MODEL_ROUTE_TABLE`

---

## ğŸ¤ Contributors / ê¸°ì—¬ì

Thanks to all the people who have contributed to this project! / ì´ í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•´ì£¼ì‹  ëª¨ë“  ë¶„ë“¤ê»˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤!

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!

**Want to contribute?** / **ê¸°ì—¬í•˜ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?**
- ğŸ› Report bugs / ë²„ê·¸ ë¦¬í¬íŠ¸
- ğŸ’¡ Suggest features / ê¸°ëŠ¥ ì œì•ˆ  
- ğŸ“– Improve documentation / ë¬¸ì„œ ê°œì„ 
- ğŸ”§ Submit code / ì½”ë“œ ê¸°ì—¬
- ğŸŒ Translate / ë²ˆì—­ ì‘ì—…

<!-- markdownlint-restore -->
<!-- prettier-ignore-end -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

### How to add contributors automatically / ìë™ ê¸°ì—¬ì ì¶”ê°€ ë°©ë²•

ì´ í”„ë¡œì íŠ¸ëŠ” [All Contributors Bot](https://allcontributors.org/)ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ì—¬ì ë¬¸ì„œí™”ë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤:

1. **Bot ì„¤ì¹˜**: [All Contributors App](https://github.com/apps/allcontributors) GitHub App ì„¤ì¹˜
2. **ê¸°ì—¬ì ì¶”ê°€**: ëŒ“ê¸€ë¡œ `@all-contributors please add @username for code`
3. **ìë™ ì—…ë°ì´íŠ¸**: Botì´ ìë™ìœ¼ë¡œ READMEì™€ `.all-contributorsrc` íŒŒì¼ ì—…ë°ì´íŠ¸

**ê¸°ì—¬ ìœ í˜• / Contribution Types:**
- `code` - ì½”ë“œ ê¸°ì—¬
- `doc` - ë¬¸ì„œ ê¸°ì—¬  
- `design` - ë””ìì¸ ê¸°ì—¬
- `test` - í…ŒìŠ¤íŠ¸ ê¸°ì—¬
- `bug` - ë²„ê·¸ ë¦¬í¬íŠ¸
- `ideas` - ì•„ì´ë””ì–´ ì œì•ˆ
- `review` - ì½”ë“œ ë¦¬ë·°

---

Completion summary
- App streams JSONL and delegates RAG orchestration to n8n
- Paths and filenames match this repo (`compose/portainer-stack.yml`, `workflows/rag-*.json`)
- Includes deployment, configuration, API contract, and a PowerShell-friendly test
